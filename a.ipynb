{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d368886d3a52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# 이미지의 각 채널을 분리합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mblue_channel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreen_channel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mred_channel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# 각 채널의 값을 확인합니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "path = './train/self_supervised/'\n",
    "file_path = os.listdir('./train/self_supervised/')\n",
    "\n",
    "image = cv2.imread(path+file_path[0], 0)\n",
    "if image is not None:\n",
    "    # 이미지의 각 채널을 분리합니다.\n",
    "    blue_channel, green_channel, red_channel = cv2.split(image)\n",
    "\n",
    "    # 각 채널의 값을 확인합니다.\n",
    "    print(\"Blue Channel:\")\n",
    "    print(blue_channel)\n",
    "\n",
    "    print(\"\\nGreen Channel:\")\n",
    "    print(green_channel)\n",
    "\n",
    "    print(\"\\nRed Channel:\")\n",
    "    print(red_channel)\n",
    "\n",
    "    # 예를 들어, 각 채널의 평균 값을 출력할 수도 있습니다.\n",
    "    print(\"\\nMean Value - Blue Channel:\", np.mean(blue_channel))\n",
    "    print(\"Mean Value - Green Channel:\", np.mean(green_channel))\n",
    "    print(\"Mean Value - Red Channel:\", np.mean(red_channel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n"
     ]
    }
   ],
   "source": [
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(np.min(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import os\n",
    "data_path = './train/'\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "dataset_train = datasets.ImageFolder(os.path.join(data_path), transform=transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8678, -0.8849, -0.9020,  ...,  1.0502,  0.8961,  0.7419],\n",
      "         [-0.8678, -0.8849, -0.9020,  ...,  1.0502,  0.8961,  0.7419],\n",
      "         [-0.8507, -0.8507, -0.8678,  ...,  1.1015,  0.9817,  0.8276],\n",
      "         ...,\n",
      "         [-1.2617, -1.2445, -1.2445,  ..., -0.9705, -1.0733, -1.1418],\n",
      "         [-1.2959, -1.2788, -1.2274,  ..., -0.9705, -1.0904, -1.1932],\n",
      "         [-1.3644, -1.3302, -1.2959,  ..., -0.9363, -1.0219, -1.1760]],\n",
      "\n",
      "        [[-0.7577, -0.7752, -0.7927,  ...,  1.2031,  1.0455,  0.8880],\n",
      "         [-0.7577, -0.7752, -0.7927,  ...,  1.2031,  1.0455,  0.8880],\n",
      "         [-0.7402, -0.7402, -0.7577,  ...,  1.2556,  1.1331,  0.9755],\n",
      "         ...,\n",
      "         [-1.1604, -1.1429, -1.1429,  ..., -0.8627, -0.9678, -1.0378],\n",
      "         [-1.1954, -1.1779, -1.1253,  ..., -0.8627, -0.9853, -1.0903],\n",
      "         [-1.2654, -1.2304, -1.1954,  ..., -0.8277, -0.9153, -1.0728]],\n",
      "\n",
      "        [[-0.5321, -0.5495, -0.5670,  ...,  1.4200,  1.2631,  1.1062],\n",
      "         [-0.5321, -0.5495, -0.5670,  ...,  1.4200,  1.2631,  1.1062],\n",
      "         [-0.5147, -0.5147, -0.5321,  ...,  1.4722,  1.3502,  1.1934],\n",
      "         ...,\n",
      "         [-0.9330, -0.9156, -0.9156,  ..., -0.6367, -0.7413, -0.8110],\n",
      "         [-0.9678, -0.9504, -0.8981,  ..., -0.6367, -0.7587, -0.8633],\n",
      "         [-1.0376, -1.0027, -0.9678,  ..., -0.6018, -0.6890, -0.8458]]])\n",
      "tensor(-1.3644)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "print(dataset_train[0][0])\n",
    "print(torch.min(dataset_train[0][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
